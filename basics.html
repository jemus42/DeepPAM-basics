<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Lukas" />


<title>DeepPAM Basics</title>

<script src="basics_files/header-attrs-2.8/header-attrs.js"></script>
<script src="basics_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="basics_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="basics_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="basics_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="basics_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="basics_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="basics_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="basics_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="basics_files/navigation-1.1/tabsets.js"></script>
<link href="basics_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="basics_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "î‰™";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "î‰™";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">



<h1 class="title toc-ignore">DeepPAM Basics</h1>
<h4 class="author">Lukas</h4>
<h4 class="date">2021-06-07 18:13:07</h4>

</div>


<pre class="r"><code>knitr::opts_chunk$set(
  message=FALSE, error=TRUE, warning=FALSE
)

# Modeling
library(keras)
library(mgcv)

# Convenience
library(dplyr)
library(broom)
library(ggplot2)

# Sacrificing a goat to the python env
reticulate::use_condaenv(&quot;r-reticulate&quot;, required = TRUE)
reticulate:::ensure_python_initialized()
tensorflow::tf_config()</code></pre>
<pre><code>## TensorFlow v2.4.0 (~/Library/r-miniconda/envs/r-reticulate/lib/python3.7/site-packages/tensorflow)
## Python v3.7 (~/Library/r-miniconda/envs/r-reticulate/bin/python)</code></pre>
<div id="linear-models" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Linear Models</h1>
<pre class="r"><code>set.seed(17)
n &lt;- 700
x1 &lt;- runif(n, 0, 10)
x2 &lt;- runif(n, -2, 2)
x3 &lt;- sample(c(0, 1), n, replace = TRUE)

y &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x3 + rnorm(n, 0, 1)
df &lt;- data.frame(y, x1, x2, x3)</code></pre>
<div id="lm" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> <code>lm()</code></h2>
<pre class="r"><code>lm_lm &lt;- lm(y ~ x1 + x2 + x3, data = df)

tidy(lm_lm)</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)     1.02    0.0894      11.5 5.41e- 28
## 2 x1              2.00    0.0136     147.  0        
## 3 x2              3.01    0.0333      90.1 0        
## 4 x3              3.96    0.0786      50.3 3.22e-234</code></pre>
<pre class="r"><code>glance(lm_lm)</code></pre>
<pre><code>## # A tibble: 1 x 12
##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.978         0.978  1.04    10456.       0     3 -1018. 2045. 2068.
## # â€¦ with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
</div>
<div id="keras" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> <code>keras</code></h2>
<ul>
<li>Single layer NN</li>
<li>Linear regression layer with bias</li>
</ul>
<pre class="r"><code>lm_nn &lt;- keras_model_sequential(name = &quot;LM&quot;) %&gt;%
  layer_dense(units = 1, activation = &quot;linear&quot;, use_bias = TRUE, name = &quot;Regression&quot;)

lm_nn %&gt;%
  compile(
    loss = &quot;mse&quot;,
    optimizer = optimizer_rmsprop(),
    metrics = NULL
  )

lm_nn %&gt;%
  fit(
    as.matrix(df[, 2:4]), df$y,
    epochs = 50,
    batch_size = 8,
    validation_split = 0.2,
    verbose = TRUE
  )

lm_nn</code></pre>
<pre><code>## Model
## Model: &quot;LM&quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## Regression (Dense)                  (None, 1)                       4           
## ================================================================================
## Total params: 4
## Trainable params: 4
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<pre class="r"><code>get_weights(lm_nn)</code></pre>
<pre><code>## [[1]]
##          [,1]
## [1,] 1.932651
## [2,] 2.759862
## [3,] 1.899272
## 
## [[2]]
## [1] 2.411098</code></pre>
<p>Close enough?</p>
</div>
</div>
<div id="generalised-additive-models" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Generalised Additive Models</h1>
<pre class="r"><code>set.seed(17)
n &lt;- 700
x1 &lt;- runif(n, 0, 10)
x2 &lt;- runif(n, -2, 2)
x3 &lt;- sample(c(0, 1), n, replace = TRUE)

predictor &lt;- -3 + 0.05 * x1^2 + x2 + x3
prob &lt;- exp(predictor) / (1 + exp(predictor))
y &lt;- rbinom(n, 1, prob)
df &lt;- data.frame(y, x1, x2, x3)</code></pre>
<div id="mgcv" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> <code>mgcv()</code></h2>
<blockquote>
<p>Use the mgcv package to estimate a GAM with linear effects of x2 and x3 and a smooth x1 effect.</p>
</blockquote>
<pre class="r"><code>gam_mgcv &lt;- gam(y ~ s(x1, fx = FALSE) + x2 + x3, family = binomial(link = &quot;logit&quot;), data = df)

gam_mgcv$coefficients</code></pre>
<pre><code>##  (Intercept)           x2           x3      s(x1).1      s(x1).2      s(x1).3 
## -1.403222857  1.150180559  1.185939615  0.002580073 -0.211213984  0.047243729 
##      s(x1).4      s(x1).5      s(x1).6      s(x1).7      s(x1).8      s(x1).9 
## -0.165042227  0.056723938 -0.185237557  0.033842307 -0.910089541  1.431656046</code></pre>
<pre class="r"><code># smoothing param for later
gam_mgcv$sp</code></pre>
<pre><code>##    s(x1) 
## 1.397658</code></pre>
<pre class="r"><code># S matrix
gam_mgcv$smooth[[1]]$S[[1]]</code></pre>
<pre><code>##                [,1]          [,2]          [,3]          [,4]          [,5]
##  [1,]  6.827540e+00 -9.348319e-01 -3.542959e+00  1.065734e+00 -4.579464e+00
##  [2,] -9.348319e-01  1.220239e+01  5.628796e-01 -6.293468e+00  4.042009e+00
##  [3,] -3.542959e+00  5.628796e-01  5.319356e+01  2.386721e+00 -5.101339e+00
##  [4,]  1.065734e+00 -6.293468e+00  2.386721e+00  5.292410e+01  6.499267e+00
##  [5,] -4.579464e+00  4.042009e+00 -5.101339e+00  6.499267e+00  1.214082e+02
##  [6,]  3.732978e+00 -1.349754e+01  4.730126e+00 -1.916964e+01  9.612156e+00
##  [7,] -5.340484e+00  4.699940e+00 -5.482847e+00  6.069105e+00 -6.533921e+00
##  [8,] -1.219789e+00  4.511026e-01  7.640241e-01 -6.278076e+00  5.199792e+00
##  [9,]  9.773332e-17 -3.614376e-17 -6.121603e-17  5.030193e-16 -4.166238e-16
##                [,6]          [,7]          [,8]          [,9]
##  [1,]  3.732978e+00 -5.340484e+00 -1.219789e+00  9.773332e-17
##  [2,] -1.349754e+01  4.699940e+00  4.511026e-01 -3.614376e-17
##  [3,]  4.730126e+00 -5.482847e+00  7.640241e-01 -6.121603e-17
##  [4,] -1.916964e+01  6.069105e+00 -6.278076e+00  5.030193e-16
##  [5,]  9.612156e+00 -6.533921e+00  5.199792e+00 -4.166238e-16
##  [6,]  1.092277e+02  7.561453e+00 -1.597419e+01  1.279903e-15
##  [7,]  7.561453e+00  3.089331e+02  6.609221e+00 -5.295517e-16
##  [8,] -1.597419e+01  6.609221e+00  7.665869e+00 -6.142137e-16
##  [9,]  1.279903e-15 -5.295517e-16 -6.142137e-16  4.921274e-32</code></pre>
<p>Takeaway: I need to do some GAM-reading.</p>
</div>
<div id="keras-1" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> <code>keras</code></h2>
<blockquote>
<p>Neglect the penalisation of the smooth eï¬€ect and construct an equivalent neural network with keras and tensorflow. Fit it.</p>
</blockquote>
<ul>
<li>Two parallel layers for linear / smooth effects? ðŸ¤”</li>
<li><a href="https://keras.rstudio.com/articles/functional_api.html">Relevant Keras docs maybe</a></li>
</ul>
<pre class="r"><code># Define inputs, two linear + 1 smooth effect
linear_input &lt;- layer_input(shape = c(2), name = &quot;linear_in&quot;)
smooth_input &lt;- layer_input(shape = c(9), name = &quot;smooth_in&quot;)

# Define intermediate outputs
linear_out &lt;- linear_input %&gt;%
  layer_dense(units = 1, activation = &quot;linear&quot;, name = &quot;linear_effect&quot;)

# Until I figure out what to use for a smooth effect here
# mgcv shows 9 components of the smooth effects, but just 1 neuron though
smooth_out &lt;- smooth_input %&gt;%
  layer_dense(units = 1, activation = &quot;linear&quot;, name = &quot;smooth_effect&quot;, use_bias = FALSE)

# layer_concatenate sums up intermediate outputs
# sigmoid activation for binary outcome
combined_out &lt;- layer_add(c(linear_out, smooth_out)) %&gt;%
  layer_activation(activation = &quot;sigmoid&quot;, name = &quot;output_prob&quot;)

# Combine the above to a keras model
gam_nn &lt;- keras_model(
  inputs = c(linear_input, smooth_input),
  outputs = c(combined_out)
)

gam_nn %&gt;%
  compile(
    loss = &quot;binary_crossentropy&quot;, # That&#39;s Bernoulli logLik right?
    optimizer = optimizer_rmsprop(),
    metrics = NULL
  )

gam_nn</code></pre>
<pre><code>## Model
## Model: &quot;model&quot;
## ________________________________________________________________________________
## Layer (type)              Output Shape      Param #  Connected to               
## ================================================================================
## linear_in (InputLayer)    [(None, 2)]       0                                   
## ________________________________________________________________________________
## smooth_in (InputLayer)    [(None, 9)]       0                                   
## ________________________________________________________________________________
## linear_effect (Dense)     (None, 1)         3        linear_in[0][0]            
## ________________________________________________________________________________
## smooth_effect (Dense)     (None, 1)         9        smooth_in[0][0]            
## ________________________________________________________________________________
## add (Add)                 (None, 1)         0        linear_effect[0][0]        
##                                                      smooth_effect[0][0]        
## ________________________________________________________________________________
## output_prob (Activation)  (None, 1)         0        add[0][0]                  
## ================================================================================
## Total params: 12
## Trainable params: 12
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<pre class="r"><code>gam_nn %&gt;%
  fit(
    # First linear input (x2, x3), then smooth input (x1)
    x = list(as.matrix(df[, 3:4]), model.matrix(gam_mgcv)[, 4:12]),
    y = df$y,
    epochs = 200,
    batch_size = 32,
    validation_split = 0.2,
    verbose = TRUE
  )

get_weights(gam_nn)</code></pre>
<pre><code>## [[1]]
##           [,1]
## [1,] 1.1195856
## [2,] 0.7238031
## 
## [[2]]
## [1] -1.017889
## 
## [[3]]
##              [,1]
##  [1,]  0.52285141
##  [2,]  0.28750712
##  [3,]  0.41107199
##  [4,]  0.18188684
##  [5,]  0.16799061
##  [6,] -0.05286731
##  [7,]  0.43142590
##  [8,] -0.23235863
##  [9,] -0.10990276</code></pre>
</div>
<div id="plot-comparion" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Plot comparion</h2>
<blockquote>
<p>Use the plot function to plot the mgcv modelâ€™s smooth term.</p>
</blockquote>
<pre class="r"><code>plot(gam_mgcv, select = 1, resid = TRUE, pch = 1, cex = 1, shade = TRUE)</code></pre>
<p><img src="basics_files/figure-html/gam-mgcv-plot-1.png" width="672" /></p>
<blockquote>
<p>Create an equivalent plot for the neural networkâ€™s smooth term. You will need to predict over a grid of x1. Bring both plots together in a single plot.</p>
</blockquote>
<pre class="r"><code>test_df &lt;- tibble::tibble(
  y = rep(0, 100),
  x1 = seq(-10, 10, length.out = 100),
  x2 = rep(0, 100),
  x3 = rep(0, 100)
)

lpmat &lt;- predict(gam_mgcv, newdata = test_df, type = &quot;lpmatrix&quot;)
gam_nn_preds &lt;- predict(gam_nn, x = list(as.matrix(lpmat[, 2:3]), lpmat[, 4:12]))

test_df %&gt;%
  mutate(yhat = as.numeric(gam_nn_preds)) %&gt;%
  ggplot(aes(x = x1, y = yhat)) + 
  geom_point() +
  geom_path() +
  theme_minimal()</code></pre>
<p><img src="basics_files/figure-html/gam-keras-plot-1.png" width="672" /></p>
</div>
<div id="keras-w-penalisation" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> <code>keras</code> w/ Penalisation</h2>
<blockquote>
<p>Read <a href="https://m-clark.github.io/generalized-additive-models/technical.html">here</a> about the penalisation of smooth terms in mgcv. Search through the mgcv model for all components of the penalisation.</p>
</blockquote>
<blockquote>
<p>Apply the proper penalisation to the equivalent neural network and refitt the network. Compare the resulting smooth terms with meaningful plots (see 3)). Use the extracted components from 4) You will find the code below helpful.</p>
</blockquote>
<pre class="r"><code>kernel_regularizer = function(x) {
  k_mean(k_batch_dot(x, k_dot(
  tf$constant(???, dtype = &quot;float64&quot;), x), axes = 2))
}</code></pre>
<p>From Wood 2017:</p>
<p><span class="math display">\[||\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}||^2 + \underbrace{\lambda \boldsymbol{\beta}^\top \boldsymbol{S} \boldsymbol{\beta}}_\text{Regularization term to reconstruct}\]</span></p>
<p>Current regularizer form:</p>
<p><span class="math display">\[\begin{aligned}
\mathtt{mean(batch\_dot(x, dot(???, x))) } &amp; \overset{?}{=}
\frac{1}{n} \sum \sum_{\mathrm{batch}} \boldsymbol{\beta} \cdot (\lambda \boldsymbol{S} \cdot \boldsymbol{\beta})
\end{aligned}\]</span></p>
<p>Making a D and S matrix</p>
<pre class="r"><code># make_Dmat &lt;-function(k = 8) {
#   diff(diag(k), differences = 2)
# }
# # S := D^T D
# make_Smat &lt;- function(k = 8) t(make_Dmat(k)) %*% make_Dmat(k)

# Or just take the one from mgcv as I&#39;m presumably supposed to
# Also multiply with lambda for convenience
gam_Smat &lt;- gam_mgcv$sp * gam_mgcv$smooth[[1]]$S[[1]]</code></pre>
<pre class="r"><code>smooth_out_reg &lt;- smooth_input %&gt;%
  layer_dense(
    units = 1, activation = &quot;linear&quot;, use_bias = FALSE, name = &quot;smooth_effect&quot;,
    kernel_regularizer = function(x) {
      k_mean(k_batch_dot(x,
        k_dot(
          tensorflow::tf$constant(gam_Smat, dtype = &quot;float32&quot;),
          x
        ),
      axes = 2
      ))
    }
  )

combined_out_reg &lt;- layer_add(c(linear_out, smooth_out_reg)) %&gt;%
  layer_activation(activation = &quot;sigmoid&quot;, name = &quot;output_prob&quot;)

# Combine the above to a keras model
gam_nn_reg &lt;- keras_model(
  inputs = c(linear_input, smooth_input),
  outputs = c(combined_out_reg)
)</code></pre>
<p>Does it do things?</p>
<pre class="r"><code>gam_nn_reg %&gt;%
  compile(
    loss = &quot;binary_crossentropy&quot;, # That&#39;s Bernoulli logLik right?
    optimizer = optimizer_rmsprop(),
    metrics = NULL
  )

gam_nn_reg</code></pre>
<pre><code>## Model
## Model: &quot;model_1&quot;
## ________________________________________________________________________________
## Layer (type)              Output Shape      Param #  Connected to               
## ================================================================================
## linear_in (InputLayer)    [(None, 2)]       0                                   
## ________________________________________________________________________________
## smooth_in (InputLayer)    [(None, 9)]       0                                   
## ________________________________________________________________________________
## linear_effect (Dense)     (None, 1)         3        linear_in[0][0]            
## ________________________________________________________________________________
## smooth_effect (Dense)     (None, 1)         9        smooth_in[0][0]            
## ________________________________________________________________________________
## add_1 (Add)               (None, 1)         0        linear_effect[0][0]        
##                                                      smooth_effect[0][0]        
## ________________________________________________________________________________
## output_prob (Activation)  (None, 1)         0        add_1[0][0]                
## ================================================================================
## Total params: 12
## Trainable params: 12
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<pre class="r"><code>gam_nn_reg %&gt;%
  fit(
    # First linear input (x2, x3), then smooth input (x1)
    x = list(as.matrix(df[, 3:4]), model.matrix(gam_mgcv)[, 4:12]),
    y = df$y,
    epochs = 200,
    batch_size = 16,
    validation_split = 0.2,
    verbose = TRUE
  )

get_weights(gam_nn_reg)</code></pre>
<pre><code>## [[1]]
##          [,1]
## [1,] 1.142495
## [2,] 1.054271
## 
## [[2]]
## [1] -1.279392
## 
## [[3]]
##                [,1]
##  [1,]  0.0019381738
##  [2,] -0.0035180959
##  [3,]  0.0003470610
##  [4,] -0.0040851720
##  [5,]  0.0005828778
##  [6,] -0.0039991857
##  [7,] -0.0001550479
##  [8,] -0.0171021596
##  [9,]  1.3893942833</code></pre>
<p>What do the predictions look like?</p>
<pre class="r"><code>gam_nn_reg_preds &lt;- predict(gam_nn_reg, x = list(as.matrix(lpmat[, 2:3]), lpmat[, 4:12]))

test_df %&gt;%
  select(x1) %&gt;%
  mutate(
    unregularized = as.numeric(gam_nn_preds),
    regularized = as.numeric(gam_nn_reg_preds)
  ) %&gt;%
  tidyr::pivot_longer(cols = ends_with(&quot;ized&quot;)) %&gt;%
  ggplot(aes(x = x1, y = value, color = name)) + 
  scale_color_brewer(palette = &quot;Dark2&quot;) +
  geom_path(size = 2) +
  labs(
    title = &quot;Regularized and unregularized smooth effects fit as NN&quot;,
    x = &quot;x1&quot;, y = &quot;s(x)&quot;, color = &quot;&quot;
    ) +
  theme_minimal() +
  theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="basics_files/figure-html/gam-keras-plots-1.png" width="672" /></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
